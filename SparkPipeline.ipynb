{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !python -m spacy download fr_core_news_sm\n",
    "# !python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create spark session on local machine\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .config(\"spark.driver.bindAddress\",\"127.0.0.1\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ReviewClass import Review\n",
    "\n",
    "spark.sparkContext.addPyFile('ReviewClass.py')\n",
    "spark.sparkContext.addPyFile('data_cleaning.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data from csv files into dataframe\n",
    "df1 = pd.read_csv('reviews/train-1.csv')\n",
    "df2 = pd.read_csv('reviews/train-2.csv')\n",
    "df3 = pd.read_csv('reviews/train-3.csv')\n",
    "df4 = pd.read_csv('reviews/train-4.csv')\n",
    "df5 = pd.read_csv('reviews/train-5.csv')\n",
    "df6 = pd.read_csv('reviews/train-6.csv')\n",
    "df7 = pd.read_csv('reviews/train-7.csv')\n",
    "df8 = pd.read_csv('reviews/train-8.csv')\n",
    "\n",
    "df_train = pd.concat([df1, df2, df3, df4, df5, df6, df7, df8])\n",
    "\n",
    "# Change name of first column\n",
    "df_train.rename(columns={ df_train.columns[0]: \"review_id\" }, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add data from dataframe into spark session\n",
    "reviews = [Review(**kwargs) for kwargs in df_train.to_dict(orient='records')]\n",
    "\n",
    "review_rdd = spark.sparkContext.parallelize(reviews, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PoC of total character count per category\n",
    "# total_character_count_per_product_category_id = review_rdd\\\n",
    "#     .flatMap(lambda review: [(review.product_category_id, review.ReviewBodyCharCount())])\\\n",
    "#     .reduceByKey(lambda count1, count2: count1 + count2)\\\n",
    "#     .sortByKey()\\\n",
    "#     .collect()\n",
    "\n",
    "# print(total_character_count_per_product_category_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PoC of total word count per category\n",
    "# total_word_count_per_product_category_id = review_rdd\\\n",
    "#     .flatMap(lambda review: [(review.product_category_id, review.ReviewBodyWordCount())])\\\n",
    "#     .reduceByKey(lambda count1, count2: count1 + count2)\\\n",
    "#     .sortByKey()\\\n",
    "#     .collect()\n",
    "\n",
    "# print(total_word_count_per_product_category_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PoC of tagged review bodies\n",
    "# list_of_tagged_reviews = review_rdd\\\n",
    "#     .map(lambda review: (review.review_id, review.TaggedReviewBody()))\\\n",
    "#     .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def RemovePunctuation(review):\n",
    "\n",
    "    new_body = str(review.review_body).replace('.', ' ')\n",
    "    new_body = str(new_body).replace(',', ' ')\n",
    "    new_body = re.sub(r'[^\\w\\s]', '', new_body)\n",
    "\n",
    "    review.review_body = str(new_body)\n",
    "\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "\n",
    "def RemoveASCII(review):\n",
    "\n",
    "    review.review_headline = html.unescape(str(review.review_headline))\n",
    "    review.review_body = html.escape(str(review.review_body))\n",
    "\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveBreaklines(review):\n",
    "\n",
    "    review.review_headline = str(review.review_headline).replace('<br />', '')\n",
    "    review.review_body = str(review.review_body).replace('<br />', '')\n",
    "\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def replace_acute_accents(text, accent_map):\n",
    "    for [accent, char] in accent_map:\n",
    "        text = re.sub(accent, char, text)\n",
    "    return text\n",
    "\n",
    "def RemoveAccentsFromBody(review):\n",
    "    \n",
    "    acute_map = np.array([['á', 'a'], ['Á', 'A'], ['é', 'e'], \n",
    "                          ['É', 'E'], ['ớ', 'o'], ['ó', 'o'], \n",
    "                          ['Ó', 'O'], ['ú', 'u'], ['Ú', 'U']])\n",
    "\n",
    "    review.review_body = replace_acute_accents(review.review_body, acute_map)\n",
    "\n",
    "    return review\n",
    "\n",
    "def RemoveAccentsFromHeadline(review):\n",
    "    \n",
    "    acute_map = np.array([['á', 'a'], ['Á', 'A'], ['é', 'e'], \n",
    "                          ['É', 'E'], ['ớ', 'o'], ['ó', 'o'], \n",
    "                          ['Ó', 'O'], ['ú', 'u'], ['Ú', 'U']])\n",
    "\n",
    "    review.review_headline = replace_acute_accents(review.review_headline, acute_map)\n",
    "\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def RemoveStopwords(review):\n",
    "\n",
    "    if review.ReviewBodyWordCount() < 5:\n",
    "        return review\n",
    "\n",
    "    if review.marketplace_id == 2:\n",
    "        stop_words = stopwords.words('french')\n",
    "    elif review.marketplace_id == 3:\n",
    "        stop_words = stopwords.words('german')\n",
    "    else:\n",
    "        stop_words = stopwords.words('english')\n",
    "\n",
    "    new_body = [word for word in word_tokenize(str(review.review_body)) if not word in stop_words]\n",
    "\n",
    "    a_str = \" \"\n",
    "\n",
    "    review.review_body = a_str.join(new_body)\n",
    "\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def Stemming(review):\n",
    "    \n",
    "    new_body = [WordNetLemmatizer().lemmatize(word) for word in word_tokenize(str(review.review_body))]\n",
    "\n",
    "    a_str = \" \"\n",
    "\n",
    "    review.review_body = a_str.join(new_body)\n",
    "\n",
    "    return review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lower(review):\n",
    "\n",
    "    review.review_body = str(review.review_body).lower()\n",
    "\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def BagOfNTopWords(review, n):\n",
    "\n",
    "    if review.ReviewBodyWordCount() < 5:\n",
    "        return review\n",
    "\n",
    "    countVec = CountVectorizer(ngram_range=(1,1))\n",
    "\n",
    "    result = countVec.fit_transform([review.review_body]).toarray()[0]\n",
    "    features = np.array(countVec.get_feature_names())\n",
    "\n",
    "    bag = []\n",
    "\n",
    "    for r, f in np.c_[result, features]:\n",
    "        bag.append((r, f))\n",
    "\n",
    "    bag.sort(key = lambda b: b[0], reverse = True)\n",
    "\n",
    "    top_n = bag[0:n]\n",
    "\n",
    "    words = [t[1] for t in top_n]\n",
    "\n",
    "    a_str = \" \"\n",
    "\n",
    "    review.bag_of_words_top_10 = a_str.join(words)\n",
    "\n",
    "    return review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ScaledPos(review):\n",
    "\n",
    "    review.TaggedReviewBody()\n",
    "\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NumOfChar(review):\n",
    "    \n",
    "    review.num_of_char = review.ReviewBodyCharCount()\n",
    "\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NumOfWords(review):\n",
    "\n",
    "    review.num_of_words = review.ReviewBodyWordCount()\n",
    "\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_cleaning import sent_tokenize\n",
    "\n",
    "def sent_count(text):\n",
    "    return len(sent_tokenize(text))\n",
    "\n",
    "def AvgSentLength(review):\n",
    "    sents = sent_tokenize(review.review_body)\n",
    "    words = word_tokenize(review.review_body)\n",
    "    \n",
    "    review.avg_sent_length = len(words) / len(sents)\n",
    "\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_reviews = review_rdd\\\n",
    "    .filter(lambda review: review.review_date == review.review_date)\\\n",
    "    .map(lambda review: AvgSentLength(review))\\\n",
    "    .map(lambda review: RemovePunctuation(review))\\\n",
    "    .map(lambda review: RemoveASCII(review))\\\n",
    "    .map(lambda review: RemoveBreaklines(review))\\\n",
    "    .map(lambda review: RemoveAccentsFromHeadline(review))\\\n",
    "    .map(lambda review: RemoveAccentsFromBody(review))\\\n",
    "    .map(lambda review: RemoveStopwords(review))\\\n",
    "    .map(lambda review: Stemming(review))\\\n",
    "    .map(lambda review: Lower(review))\\\n",
    "    .map(lambda review: BagOfNTopWords(review, 10))\\\n",
    "    .map(lambda review: ScaledPos(review))\\\n",
    "    .map(lambda review: NumOfChar(review))\\\n",
    "    .map(lambda review: NumOfWords(review))\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame([o.__dict__ for o in cleaned_reviews])\n",
    "\n",
    "df.to_csv(\"cleaned_reviews.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96f9418e69eaeb26f09100da13ed59566d98b4e8252c70eed69f1604a08955be"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
