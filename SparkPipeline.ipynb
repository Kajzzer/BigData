{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U spacy\n",
    "# %pip install deep-translator\n",
    "# %pip install langdetect\n",
    "# %pip install torchvision\n",
    "# %pip install transformers\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !python -m spacy download fr_core_news_sm\n",
    "# !python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/25 16:36:30 WARN Utils: Your hostname, DimitriH02.local resolves to a loopback address: 127.0.0.1; using 10.0.0.2 instead (on interface en7)\n",
      "22/03/25 16:36:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/dimitrihooftman/opt/anaconda3/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/25 16:36:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create spark session on local machine\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .config(\"spark.driver.bindAddress\",\"127.0.0.1\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ReviewClass import Review\n",
    "\n",
    "spark.sparkContext.addPyFile('ReviewClass.py')\n",
    "spark.sparkContext.addPyFile('data_cleaning.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data from csv files into dataframe\n",
    "# df_train = pd.read_csv('reviews/validation_hidden.csv')\n",
    "df1 = pd.read_csv('reviews/train-1.csv')\n",
    "df2 = pd.read_csv('reviews/train-2.csv')\n",
    "df3 = pd.read_csv('reviews/train-3.csv')\n",
    "df4 = pd.read_csv('reviews/train-4.csv')\n",
    "df5 = pd.read_csv('reviews/train-5.csv')\n",
    "df6 = pd.read_csv('reviews/train-6.csv')\n",
    "df7 = pd.read_csv('reviews/train-7.csv')\n",
    "df8 = pd.read_csv('reviews/train-8.csv')\n",
    "\n",
    "df_train = pd.concat([df1, df2, df3, df4, df5, df6, df7, df8])\n",
    "\n",
    "# Change name of first column\n",
    "df_train.rename(columns={ df_train.columns[0]: \"review_id\" }, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add data from dataframe into spark session\n",
    "reviews = [Review(**kwargs) for kwargs in df_train.head(2000).to_dict(orient='records')]\n",
    "\n",
    "review_rdd = spark.sparkContext.parallelize(reviews, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PoC of total character count per category\n",
    "# total_character_count_per_product_category_id = review_rdd\\\n",
    "#     .flatMap(lambda review: [(review.product_category_id, review.ReviewBodyCharCount())])\\\n",
    "#     .reduceByKey(lambda count1, count2: count1 + count2)\\\n",
    "#     .sortByKey()\\\n",
    "#     .collect()\n",
    "\n",
    "# print(total_character_count_per_product_category_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PoC of total word count per category\n",
    "# total_word_count_per_product_category_id = review_rdd\\\n",
    "#     .flatMap(lambda review: [(review.product_category_id, review.ReviewBodyWordCount())])\\\n",
    "#     .reduceByKey(lambda count1, count2: count1 + count2)\\\n",
    "#     .sortByKey()\\\n",
    "#     .collect()\n",
    "\n",
    "# print(total_word_count_per_product_category_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PoC of tagged review bodies\n",
    "# list_of_tagged_reviews = review_rdd\\\n",
    "#     .map(lambda review: (review.review_id, review.TaggedReviewBody()))\\\n",
    "#     .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def RemovePunctuation(review):\n",
    "\n",
    "    new_body = str(review.review_body).replace('.', ' ')\n",
    "    new_body = str(new_body).replace(',', ' ')\n",
    "    new_body = re.sub(r'[^\\w\\s]', '', new_body)\n",
    "\n",
    "    review.review_body = str(new_body)\n",
    "\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "\n",
    "def RemoveASCII(review):\n",
    "\n",
    "    review.review_headline = html.unescape(str(review.review_headline))\n",
    "    review.review_body = html.escape(str(review.review_body))\n",
    "\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveBreaklines(review):\n",
    "\n",
    "    review.review_headline = str(review.review_headline).replace('<br />', '')\n",
    "    review.review_body = str(review.review_body).replace('<br />', '')\n",
    "\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def replace_acute_accents(text, accent_map):\n",
    "    for [accent, char] in accent_map:\n",
    "        text = re.sub(accent, char, text)\n",
    "    return text\n",
    "\n",
    "def RemoveAccentsFromBody(review):\n",
    "    \n",
    "    acute_map = np.array([['á', 'a'], ['Á', 'A'], ['é', 'e'], \n",
    "                          ['É', 'E'], ['ớ', 'o'], ['ó', 'o'], \n",
    "                          ['Ó', 'O'], ['ú', 'u'], ['Ú', 'U']])\n",
    "\n",
    "    review.review_body = replace_acute_accents(review.review_body, acute_map)\n",
    "\n",
    "    return review\n",
    "\n",
    "def RemoveAccentsFromHeadline(review):\n",
    "    \n",
    "    acute_map = np.array([['á', 'a'], ['Á', 'A'], ['é', 'e'], \n",
    "                          ['É', 'E'], ['ớ', 'o'], ['ó', 'o'], \n",
    "                          ['Ó', 'O'], ['ú', 'u'], ['Ú', 'U']])\n",
    "\n",
    "    review.review_headline = replace_acute_accents(review.review_headline, acute_map)\n",
    "\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from langdetect import detect\n",
    "\n",
    "def RemoveStopwords(review):\n",
    "\n",
    "    if review.ReviewBodyWordCount() < 5:\n",
    "        review.lang = 0\n",
    "        return review\n",
    "\n",
    "    if detect(review.review_body) == 'fr':\n",
    "        review.lang = 1\n",
    "        stop_words = stopwords.words('french')\n",
    "    if detect(review.review_body) == 'de':\n",
    "        review.lang = 2\n",
    "        stop_words = stopwords.words('german')\n",
    "    else:\n",
    "        review.lang = 0\n",
    "        stop_words = stopwords.words('english')\n",
    "\n",
    "    new_body = [word for word in word_tokenize(str(review.review_body)) if not word in stop_words]\n",
    "\n",
    "    a_str = \" \"\n",
    "\n",
    "    review.review_body = a_str.join(new_body)\n",
    "\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def Stemming(review):\n",
    "    \n",
    "    new_body = [WordNetLemmatizer().lemmatize(word) for word in word_tokenize(str(review.review_body))]\n",
    "\n",
    "    a_str = \" \"\n",
    "\n",
    "    review.review_body = a_str.join(new_body)\n",
    "\n",
    "    return review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lower(review):\n",
    "\n",
    "    review.review_body = str(review.review_body).lower()\n",
    "\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def BagOfNTopWords(review, n):\n",
    "\n",
    "    if review.ReviewBodyWordCount() < 5:\n",
    "        return review\n",
    "\n",
    "    countVec = CountVectorizer(ngram_range=(1,1))\n",
    "\n",
    "    result = countVec.fit_transform([review.review_body]).toarray()[0]\n",
    "    features = np.array(countVec.get_feature_names())\n",
    "\n",
    "    bag = []\n",
    "\n",
    "    for r, f in np.c_[result, features]:\n",
    "        bag.append((r, f))\n",
    "\n",
    "    bag.sort(key = lambda b: b[0], reverse = True)\n",
    "\n",
    "    top_n = bag[0:n]\n",
    "\n",
    "    words = [t[1] for t in top_n]\n",
    "\n",
    "    a_str = \" \"\n",
    "\n",
    "    review.bag_of_words_top_10 = a_str.join(words)\n",
    "\n",
    "    return review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ScaledPos(review):\n",
    "\n",
    "    review.TaggedReviewBody()\n",
    "\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NumOfChar(review):\n",
    "    \n",
    "    review.num_of_char = review.ReviewBodyCharCount()\n",
    "\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NumOfWords(review):\n",
    "\n",
    "    review.num_of_words = review.ReviewBodyWordCount()\n",
    "\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NumOfCapitals(review):\n",
    "\n",
    "    count=0\n",
    "    \n",
    "    for i in review.review_body:\n",
    "        if i.isupper():\n",
    "            count += 1\n",
    "    \n",
    "    review.num_of_capital = count\n",
    "\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NumOfCaptialWords(review):\n",
    "\n",
    "    review.num_of_capital_words = sum(map(str.isupper, review.review_body.split()))\n",
    "\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NumOfPunctuation(review):\n",
    "\n",
    "    count=0\n",
    "\n",
    "    punctuations='!\"#$%&\\'()*+,-./:;<=>?@[\\]^_`{|}~'\n",
    "\n",
    "    for i in review.review_body:\n",
    "        if i in punctuations:\n",
    "            count += 1\n",
    "\n",
    "    review.num_of_punctuation = count\n",
    "\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NumOfWordsInQuote(review):\n",
    "    x = re.findall('\".*?\"|\\'.*?\\'', review.review_body)\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    if x is not None:\n",
    "        for i in x:\n",
    "            count += 1\n",
    "\n",
    "    review.num_of_words_in_quote = count\n",
    "    \n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NumOfUniqueWord(review):\n",
    "\n",
    "    review.num_of_unique_word = len(set(review.review_body.split()))\n",
    "\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AvgWordLength(review):\n",
    "\n",
    "    review.avg_word_length = review.num_of_char / review.num_of_words\n",
    "\n",
    "\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RatioUniqueWords(review):\n",
    "\n",
    "    review.ratio_unique_words = review.num_of_unique_word / review.num_of_words\n",
    "\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RatioStopWords(review):\n",
    "\n",
    "    review.ratio_stopwords = review.num_of_stopwords / review.num_of_words\n",
    "\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NumOfStopWords(review):\n",
    "    \n",
    "    if detect(review.review_body) == 'fr':\n",
    "        stop_words = stopwords.words('french')\n",
    "    if detect(review.review_body) == 'de':\n",
    "        stop_words = stopwords.words('german')\n",
    "    else:\n",
    "        stop_words = stopwords.words('english')\n",
    "\n",
    "    stop_words = set(stop_words)\n",
    "    word_tokens = word_tokenize(review.review_body)\n",
    "    stopwords_x = [w for w in word_tokens if w in stop_words]\n",
    "\n",
    "    review.num_of_stopwords = len(stopwords_x)\n",
    "\n",
    "    return review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def Sentiment(review, classifier):\n",
    "\n",
    "    re = GoogleTranslator(source='auto', target='en').translate(review.review_body[0:500])\n",
    "\n",
    "    score = classifier(re)[0]['score']\n",
    "    label = classifier(re)[0]['label']\n",
    "\n",
    "    review.sen_score = score\n",
    "    if label == \"NEGATIVE\":\n",
    "        review.sen_label = -1\n",
    "    elif label == \"POSITIVE\":\n",
    "        review.sen_label = 1\n",
    "    else:\n",
    "        review.sen_label = 0\n",
    "\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_cleaning import sent_tokenize\n",
    "\n",
    "def sent_count(text):\n",
    "    return len(sent_tokenize(text))\n",
    "\n",
    "def AvgSentLength(review):\n",
    "    sents = sent_tokenize(review.review_body)\n",
    "    words = word_tokenize(review.review_body)\n",
    "    \n",
    "    review.avg_sent_length = len(words) / len(sents)\n",
    "\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "from langdetect import detect\n",
    "import copy\n",
    "\n",
    "def DoubleTranslate(review):\n",
    "\n",
    "    if detect(review.review_body) == 'en':\n",
    "\n",
    "        translated = GoogleTranslator(source='en', target='fr').translate(review.review_body)\n",
    "        translated_back = GoogleTranslator(source='fr', target='en').translate(translated)\n",
    "    else:\n",
    "\n",
    "        translated = GoogleTranslator(source='auto', target='en').translate(review.review_body)\n",
    "        translated_back = GoogleTranslator(source='en', target=detect(review.review_body)).translate(translated)\n",
    "\n",
    "    translated_review = copy.deepcopy(review)\n",
    "\n",
    "    translated_review.review_body = translated_back\n",
    "\n",
    "    return [review, translated_review]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n",
      "[Stage 0:===>                                                     (7 + 1) / 100]\r"
     ]
    }
   ],
   "source": [
    "classifier = pipeline('sentiment-analysis')\n",
    "\n",
    "cleaned_reviews = review_rdd\\\n",
    "    .filter(lambda review: review.review_date == review.review_date)\\\n",
    "    .map(lambda review: ScaledPos(review))\\\n",
    "    .map(lambda review: NumOfChar(review))\\\n",
    "    .map(lambda review: NumOfWords(review))\\\n",
    "    .map(lambda review: NumOfCapitals(review))\\\n",
    "    .map(lambda review: NumOfCaptialWords(review))\\\n",
    "    .map(lambda review: NumOfPunctuation(review))\\\n",
    "    .map(lambda review: NumOfWordsInQuote(review))\\\n",
    "    .map(lambda review: NumOfUniqueWord(review))\\\n",
    "    .map(lambda review: NumOfStopWords(review))\\\n",
    "    .map(lambda review: AvgWordLength(review))\\\n",
    "    .map(lambda review: AvgSentLength(review))\\\n",
    "    .map(lambda review: RatioUniqueWords(review))\\\n",
    "    .map(lambda review: RemovePunctuation(review))\\\n",
    "    .map(lambda review: RemoveASCII(review))\\\n",
    "    .map(lambda review: RemoveBreaklines(review))\\\n",
    "    .map(lambda review: RemoveAccentsFromHeadline(review))\\\n",
    "    .map(lambda review: RemoveAccentsFromBody(review))\\\n",
    "    .map(lambda review: RemoveStopwords(review))\\\n",
    "    .map(lambda review: Stemming(review))\\\n",
    "    .map(lambda review: Lower(review))\\\n",
    "    .map(lambda review: BagOfNTopWords(review, 10))\\\n",
    "    .map(lambda review: Sentiment(review, classifier))\\\n",
    "    .flatMap(lambda review: DoubleTranslate(review))\\\n",
    "    .collect()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame([o.__dict__ for o in cleaned_reviews])\n",
    "\n",
    "df.to_csv(\"cleaned_reviews_with_double_translations.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cProfile import label\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# use GPU if available, else use CPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# print(f\"Using {device} device\")\n",
    "\n",
    "\n",
    "# neural network\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, no_features):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(no_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 2),\n",
    "            # nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# train the NN\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    acc = 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        acc += flat_accuracy(pred.detach(), y.detach())\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # if batch % 100 == 0:\n",
    "        #     loss, current = loss.item(), batch * len(X)\n",
    "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    print('train accuracy = ', acc/len(dataloader))\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "# evaluate on different dataset\n",
    "def eval(dataloader, model, loss_fn):\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_size = 0\n",
    "    acc = 0\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y).item()\n",
    "        total_loss += loss * len(y)\n",
    "        total_size += len(y)\n",
    "        acc += flat_accuracy(pred.detach(), y.detach())\n",
    "    \n",
    "    total_loss /= total_size\n",
    "    print('val accuracy = ', acc/len(dataloader))\n",
    "    return total_loss\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds.numpy(), axis=1).flatten()\n",
    "    labels_flat = labels.numpy().flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "\n",
    "import torch.utils.data as data\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def Net():\n",
    "\n",
    "    # get dataframes and subtract values\n",
    "    df_train = pd.read_csv('cleaned_reviews_with_double_translations.csv')  \n",
    "\n",
    "    # features\n",
    "    sent_train = np.array(df_train['avg_sent_length'].astype(float))\n",
    "    posverb_train = np.array(df_train['pos_verb_ratio'].astype(float))\n",
    "    pospropn_train = np.array(df_train['pos_propn_ratio'].astype(float))\n",
    "    pos_aux_ratio_train = np.array(df_train['pos_aux_ratio'].astype(float))\n",
    "    pos_adp_ratio_train = np.array(df_train['pos_adp_ratio'].astype(float))\n",
    "    pos_noun_ratio_train = np.array(df_train['pos_noun_ratio'].astype(float))\n",
    "    pos_num_ratio_train = np.array(df_train['pos_num_ratio'].astype(float))\n",
    "    num_of_char_train = np.array(df_train['num_of_char'].astype(float))\n",
    "    num_of_words_train = np.array(df_train['num_of_words'].astype(float))\n",
    "    df_train['verified_purchase'] = df_train['verified_purchase'].map(lambda x: 1 if x == 'Y' else 0)\n",
    "    verified_purchase = np.array(df_train['verified_purchase'])\n",
    "    num_of_capital_train = np.array(df_train['num_of_capital'].astype(float))\n",
    "    num_of_capital_words_train = np.array(df_train['num_of_capital_words'].astype(float))\n",
    "    num_of_punctuation_train = np.array(df_train['num_of_punctuation'].astype(float))\n",
    "    num_of_words_in_quote_train = np.array(df_train['num_of_words_in_quote'].astype(float))\n",
    "    num_of_unique_word_train = np.array(df_train['num_of_unique_word'].astype(float))\n",
    "    num_of_stopwords_train = np.array(df_train['num_of_stopwords'].astype(float))\n",
    "    avg_word_length_train = np.array(df_train['avg_word_length'].astype(float))\n",
    "    ratio_unique_words_train = np.array(df_train['ratio_unique_words'].astype(float))\n",
    "    lang_train = np.nan_to_num(np.array(df_train['lang'].astype(float)), 0)\n",
    "    sen_label = np.array(df_train['sen_label'].astype(float))\n",
    "    sen_score = np.array(df_train['sen_score'].astype(float))\n",
    "\n",
    "\n",
    "    X_train = np.column_stack([verified_purchase, sent_train, posverb_train, pospropn_train, pos_aux_ratio_train ,pos_adp_ratio_train, pos_noun_ratio_train, pos_num_ratio_train, num_of_char_train, num_of_words_train, num_of_capital_train, num_of_capital_words_train, num_of_punctuation_train, num_of_words_in_quote_train, num_of_unique_word_train, num_of_stopwords_train, avg_word_length_train, ratio_unique_words_train, lang_train, sen_score, sen_label])\n",
    "    y_train = df_train['label'].astype(int)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "    train_data = list(zip(X_train, y_train))\n",
    "    valid_data = list(zip(X_val, y_val))\n",
    "\n",
    "    BATCH_SIZE = 16\n",
    "    train_data = data.DataLoader(train_data, batch_size=BATCH_SIZE, num_workers=2)\n",
    "    valid_data = data.DataLoader(valid_data, batch_size=BATCH_SIZE, num_workers=2)\n",
    "\n",
    "\n",
    "    # initialize model, loss function and optimizer\n",
    "    model = NeuralNetwork(no_features=21).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    best_total_loss = torch.inf\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    epochs = []\n",
    "\n",
    "    for epoch in range(15):\n",
    "        epochs.append(epoch)\n",
    "        print('epoch: ', epoch)\n",
    "\n",
    "        # train on training set\n",
    "        train_loss = train(train_data, model, loss_fn, optimizer)\n",
    "        train_losses.append(train_loss)\n",
    "        print('train loss: ', train_loss)\n",
    "\n",
    "        # evaluate the trained model on validation set\n",
    "        total_loss = eval(valid_data, model, loss_fn)\n",
    "        val_losses.append(total_loss)\n",
    "        print('val loss: ', total_loss)\n",
    "        print('\\n')\n",
    "\n",
    "        # if loss is lower, save new best model\n",
    "        if total_loss < best_total_loss:\n",
    "            best_total_loss = total_loss\n",
    "            torch.save(model, 'best model J')\n",
    "\n",
    "\n",
    "        # # test best model on test set\n",
    "        # best_model = torch.load('best model J')\n",
    "        # total_loss = eval(test_data, model, loss_fn)\n",
    "        # print('test_loss: ', total_loss)\n",
    "\n",
    "        # plt.plot(epochs, train_losses, label='train')\n",
    "        # plt.plot(epochs, val_losses, label='validation')\n",
    "        # plt.xlabel('epoch')\n",
    "        # plt.ylabel('loss')\n",
    "        # plt.legend()\n",
    "        # plt.show()\n",
    "\n",
    "# Net()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96f9418e69eaeb26f09100da13ed59566d98b4e8252c70eed69f1604a08955be"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
